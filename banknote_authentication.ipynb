{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTXI74Mh_Iui"
      },
      "source": [
        "# Machine Learning\n",
        "## Capstone Project\n",
        "## Project: Identification of Fake Currency Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpE3Osit_Iuq"
      },
      "source": [
        "## Introduction\n",
        "In this project, we employ several supervised machine learning algorithms to build models that distinguish between genuine and counterfeit banknotes. We will analyze these algorithms to choose the best candidate, and then try to further optimize the algorithm to best model the data. Our goal with this implementation is to accurately predict whether a currency note is genuine or fake. This sort of analysis is needed, since every major economy is flooded with counterfeit notes. \n",
        "\n",
        "The dataset for this project originates from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/banknote+authentication) The dataset was donated by  Volker Lohweg and  Helene Doerksen. Data were extracted from images that were taken from genuine and forged banknote-like specimens. Wavelet Transformation tools were used to extract features from images. We investigate here with the original dataset, manually adding only the header line comprising of feature names, as it was not present in the original file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Loading Visauls.py and CSV file in Colab Environment**(Skip in Local Environment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!curl https://raw.githubusercontent.com/rohanvh7/Fake-Currency-Identification/master/banknote_authentication.csv > banknote_authentication.csv\n",
        "!curl https://raw.githubusercontent.com/rohanvh7/Fake-Currency-Identification/master/visuals.py >visuals.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Yeq5-0H_Iuw"
      },
      "source": [
        "## Exploring the data\n",
        "We load here the necessary Python libraries and load the banknote authentication dataset. The last column of this dataset 'class' is our target variable. All other columns are features about each currency note in the dataset. The target column is a binary valued variable, with value 1 meaning the note is genuine, and value 0 meaning the note is counterfeit. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqdpBdLG_Iu1",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "\n",
        "#Import supplementary visualization code visuals.py\n",
        "import visuals as vs\n",
        "\n",
        "# Display all visuals inline\n",
        "%matplotlib inline\n",
        "\n",
        "# Load the Banknote Authentication dataset\n",
        "data = pd.read_csv(\"banknote_authentication.csv\")\n",
        "display(data.head(n=6))\n",
        "\n",
        "classes = data['class']\n",
        "features = data.drop('class', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "O5IQ4C00_Iu9"
      },
      "outputs": [],
      "source": [
        "# autoreload for automatically reloading changes made in visuals.py\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOHjCwgO_IvB"
      },
      "source": [
        "----\n",
        "## Exploratory Data Analysis\n",
        "Now, we will do some data exploration to find out the number of samples belonging to each of the two classes, that is, how many samples belong to fake notes and how many belong to genuine notes. We will also determine if the dataset contains any missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wl0I4MNT_IvE"
      },
      "outputs": [],
      "source": [
        "n_records = len(data)\n",
        "n_fake_notes = len(data[data['class'] == 0])\n",
        "n_real_notes = len(data[data['class'] == 1])\n",
        "print(\"Total number of records: {}\".format(n_records))\n",
        "print(\"Total number of fake notes: {}\".format(n_fake_notes))\n",
        "print(\"Total number of real notes: {}\".format(n_real_notes))\n",
        "\n",
        "missing_values = data.isnull().sum().sum()\n",
        "if missing_values == 0:\n",
        "    print(\"\\nThere are no missing values in the dataset\")\n",
        "else:\n",
        "    print(\"\\nThe dataset has {} missing values\".format(missing_values))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_QYSpjD_IvI"
      },
      "source": [
        "### Statistical Analysis of Features\n",
        "In the code cell below, we statistically describle various features of the dataset, including mean, standard deviation, median, minimum and maximum values. We observe that the features have different range of values, and would like to scale them in the reange of 0-1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpR-dnG2_IvL"
      },
      "outputs": [],
      "source": [
        "display(data.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5yYpSoI_IvP"
      },
      "source": [
        "### Looking for Skewedness in Continuous Features\n",
        "A dataset may sometimes contain at least one feature whose values tend to lie near a single number, but will also have a non-trivial number of vastly larger or smaller values than that single number. Algorithms can be sensitive to such distributions of values and can underperform if the range is not properly normalized. We plot distributions of all continuous features to find if any skewedness is present in any of the features. We observe that all features are well distributed and do not have any such outlier values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EWf5dHb_IvR"
      },
      "outputs": [],
      "source": [
        "# feature plotting\n",
        "vs.distribution(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmeNvHZE_IvT"
      },
      "source": [
        "### Identifying Predictive Power of Features\n",
        "We create six classification scatter plots (using 2 of the 4 numerical attributes for each display), and use different colors for objects belonging to class 1 and class 0. This will give us an idea about which pair of features classify the dataset more strongly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xY8gX-8_IvV"
      },
      "outputs": [],
      "source": [
        "vs.scatter(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJdTxJh8_IvW"
      },
      "source": [
        "----\n",
        "## Preparing the Data\n",
        "Before data can be used as input for machine learning algorithms, it often must be cleaned, formatted, and restructured — this is typically known as preprocessing. Fortunately, for this dataset, there are no invalid or missing entries we must deal with, however, there are some qualities about certain features that must be adjusted. This preprocessing can help tremendously with the outcome and predictive power of nearly all learning algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTqtcF3a_IvX"
      },
      "source": [
        "### Normalizing Numerical Features\n",
        " In this section, we want to perform some type of scaling on numerical features. Applying a scaling to the data does not change the shape of each feature's distribution (such as 'variance' or 'skewedness' above); however, normalization ensures that each feature is treated equally when applying supervised learners. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_f913vU_IvY"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "numerical = ['variance', 'skewness', 'kurtosis', 'entropy']\n",
        "scaler = MinMaxScaler()\n",
        "features[numerical] = scaler.fit_transform(features[numerical])\n",
        "\n",
        "display(features.head(n = 6))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf42oWOy_IvZ"
      },
      "source": [
        "### Shuffle and Split Data\n",
        "Now that all numerical features have been normalized, we will now split the data (both features and their labels) into training and test sets. 60% of the data will be used for training and 40% for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9rXY0tH_Iva"
      },
      "outputs": [],
      "source": [
        "# Import train_test_split\n",
        "from sklearn.model_selection import train_test_split #changed cross validation to modelselection\n",
        "\n",
        "# Split the 'features' and 'classes' data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, classes, test_size = 0.4, random_state = 5)\n",
        "\n",
        "# Show the results of the split\n",
        "print (\"Training set has {} samples.\".format(X_train.shape[0]))\n",
        "print (\"Testing set has {} samples.\".format(X_test.shape[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X2NhFGO_Ivb"
      },
      "source": [
        "----\n",
        "## Evaluating Model Performance\n",
        "In this section, we will investigate four different algorithms, and determine which is best at modeling the data. Three of these algorithms will be supervised learners, and the fourth algorithm is known as a naive predictor, which is our benchmark model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9INvvhV_Ivd"
      },
      "source": [
        "### Metrics and the Naive Predictor\n",
        "We also require to define evaluation metrics that we can use to quantify the performance of our solution, as well as, the benchmark model. The metrics that we will use for our problem are accuracy and f-score. Accuracy is defined as the number of samples correctly classified to the total number of samples. This can be a good metric, but suppose, if it’s more detrimental for us if are not able to correctly classify a fake note, than a real one. Therefore, the model’s ability to recall all fake currency notes is more important than the model’s ability to make precise prediction. In this scenario, we can use, Fβ score with β = 2, as it weighs recall more than precision. The general form is: \n",
        "$$ F_{\\beta} = (1 + \\beta^2) \\cdot \\frac{precision \\cdot recall}{\\left( \\beta^2 \\cdot precision \\right) + recall} $$\n",
        "\n",
        "We will compare the performance of our model with a naïve classifier, which classifies all banknotes as fake. The accuracy of this naïve classifier will be number of fake banknotes to the total number of notes in the data. Our model should perform far better than the naïve approach to be any worthy for intended use. Since, this naïve prediction model does not consider any information to substantiate its claim, it helps establish a benchmark for whether a model is performing well. That been said, using a naïve prediction would be pointless as will predict all notes counterfeit and would not identify any note as genuine.\n",
        "\n",
        "In the code cell below, we calculate the evaluation metrics for the Naive Predictor benchmark model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDBiKAH5_Ive"
      },
      "outputs": [],
      "source": [
        "# Calculate Accuracy\n",
        "accuracy = float(n_fake_notes)/n_records\n",
        "\n",
        "# Calculate F-Score with beta = 2\n",
        "precision = accuracy\n",
        "recall = 1\n",
        "beta = 2\n",
        "fscore = (1 + beta ** 2) * precision * recall /(beta ** 2 * precision + recall)\n",
        "\n",
        "# Print the results \n",
        "print (\"Naive Predictor: [Accuracy score: {:.4f}, F-score: {:.4f}]\".format(accuracy, fscore))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIFcoQs6_Ivg"
      },
      "source": [
        "### Supervised Learning Models\n",
        "We have chosen the following supervised learning models to build the trained models:\n",
        "- Support Vector Machines (SVM)\n",
        "- Gradient Boosting, an ensemble method\n",
        "- K-Nearest Neighbors (KNeighbors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD6ekihy_Ivh"
      },
      "source": [
        "### Implementation: Create Training and Prediction Pipeline\n",
        "To properly evaluate the performance of each model you've chosen, it's important that we create a training and predicting pipeline that allows us to quickly and effectively train models using various sizes of training data and perform predictions on the testing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpcXW60K_Ivi"
      },
      "outputs": [],
      "source": [
        "# Import two metrics from sklearn - fbeta_score and accuracy_score\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def train_predict(learner, sample_size, X_train, y_train, X_test, y_test): \n",
        "    '''\n",
        "    inputs:\n",
        "       - learner: the learning algorithm to be trained and predicted on\n",
        "       - sample_size: the size of samples (number) to be drawn from training set\n",
        "       - X_train: features training set\n",
        "       - y_train: income training set\n",
        "       - X_test: features testing set\n",
        "       - y_test: income testing set\n",
        "    '''\n",
        "    \n",
        "    results = {}\n",
        "    beta_value=2 #changed from beta to beta_value\n",
        "    # Fit the learner to the training data using slicing with 'sample_size'\n",
        "    start = time() # Get start time\n",
        "    learner.fit(X_train[:sample_size], y_train[:sample_size])\n",
        "    end = time() # Get end time\n",
        "    \n",
        "    # Calculate the training time\n",
        "    results['train_time'] = end - start\n",
        "        \n",
        "    # Get the predictions on the test set,\n",
        "    # then get predictions on the first 300 training samples\n",
        "    start = time() # Get start time\n",
        "    predictions_test = learner.predict(X_test)\n",
        "    predictions_train = learner.predict(X_train[:300])\n",
        "    end = time() # Get end time\n",
        "    \n",
        "    # Calculate the total prediction time\n",
        "    results['pred_time'] = end - start\n",
        "            \n",
        "    # Compute accuracy on the first 300 training samples\n",
        "    results['acc_train'] = accuracy_score(y_train[:300], predictions_train)\n",
        "        \n",
        "    # Compute accuracy on test set\n",
        "    results['acc_test'] = accuracy_score(y_test, predictions_test)\n",
        "    \n",
        "    # Compute F-score on the the first 300 training samples\n",
        "    results['f_train'] = fbeta_score(y_train[:300], predictions_train, beta=beta_value)#changed beta to beta_value\n",
        "        \n",
        "    # Compute F-score on the test set\n",
        "    results['f_test'] = fbeta_score(y_test, predictions_test, beta=beta_value)#changed to beta to beta_value\n",
        "    \n",
        "    # Compute Confusion Matrix\n",
        "    results['conf_mat'] = confusion_matrix(y_test, predictions_test)\n",
        "       \n",
        "    # Success\n",
        "    print (\"{} trained on {} samples.\".format(learner.__class__.__name__, sample_size))\n",
        "        \n",
        "    # Return the results\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzhwXHJC_Ivj"
      },
      "source": [
        "### Initial Model Evaluation\n",
        "In the code cell, we implement the following:\n",
        "- Import the three supervised learning models that we have chosen.\n",
        "- Initialize the three models and store them in 'clf_A', 'clf_B', and 'clf_C'.\n",
        "- Calculate the number of records equal to 5%, 20%, and 100% of the training data.\n",
        "- Store those values in 'samples_5', 'samples_20', and 'samples_100' respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Mbcf69__Ivj"
      },
      "outputs": [],
      "source": [
        "# Import the three supervised learning models from sklearn\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from math import ceil\n",
        "from time import time\n",
        "\n",
        "# Initialize the three models\n",
        "clf_A = KNeighborsClassifier(n_neighbors = 5)\n",
        "clf_B = SVC(random_state = 5)\n",
        "clf_C = GradientBoostingClassifier(random_state = 5)\n",
        "\n",
        "# Calculate the number of samples for 1%, 10%, and 100% of the training data\n",
        "samples_5 = int(X_train.shape[0] * 0.05)\n",
        "samples_20 = int(X_train.shape[0] * 0.2)\n",
        "samples_100 = X_train.shape[0]\n",
        "\n",
        "# Collect results on the learners\n",
        "results = {}\n",
        "for clf in [clf_A, clf_B, clf_C]:\n",
        "    clf_name = clf.__class__.__name__\n",
        "    results[clf_name] = {}\n",
        "    for i, samples in enumerate([samples_5, samples_20, samples_100]):\n",
        "        results[clf_name][i] = train_predict(clf, samples, X_train, y_train, X_test, y_test)\n",
        "\n",
        "print(\"\\nTrain-Test Scores when trained on entire training set\")\n",
        "for key, value in results.items():#changed to iteritems\n",
        "    print (\"For {} Classifier, scores on training and test set are:\".format(key))\n",
        "    print (\"Accuracy on Train set: {}\".format(value[2]['acc_train']))\n",
        "    print (\"Accuracy on Test set: {}\".format(value[2]['acc_test']))\n",
        "    print (\"F-score on Train set: {}\".format(value[2]['f_train']))\n",
        "    print (\"F-score on Test set: {}\".format(value[2]['f_test']))\n",
        "    print (\"Confusion Matrix for test set:\\n {}\".format(value[2]['conf_mat']))\n",
        "# Run metrics visualization for the three supervised learning models chosen\n",
        "vs.evaluate(results, accuracy, fscore)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4nVVbHr_Ivk"
      },
      "source": [
        "----\n",
        "## Improving Results\n",
        "In this final section, we choose from the three supervised learning models the *best* model to use. We will then perform a grid search optimization for the model over the entire training set (`X_train` and `y_train`) by tuning at least one parameter to improve upon the untuned model's F-score. \n",
        "\n",
        "We choose KNN-Classifier as the best model, and try to fine-tune its parameters to improvise on our results. But we see that the model is already performing optimally, and there is no visible improvement.\n",
        "\n",
        "We also output the best set of parameters that the model has identified."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqGg88NL_Ivl"
      },
      "source": [
        "### Model Tuning\n",
        "We fine-tune the chosen model. We use grid search (GridSearchCV) with at least one important parameter. We will need to use the entire training set for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "KRRGGQXO_Ivl",
        "outputId": "00582d70-5f7d-4450-8332-524efd0117ee"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-5fee7b613cfc>\"\u001b[0;36m, line \u001b[0;32m29\u001b[0m\n\u001b[0;31m    print \"Unoptimized model\\n------\"\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(\"Unoptimized model\\n------\")?\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV #changed grid_search to model_selection\n",
        "from sklearn.metrics import make_scorer, fbeta_score\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "# Initialize the classifier\n",
        "clf = KNeighborsClassifier()\n",
        "\n",
        "# Create the parameters list you wish to tune\n",
        "parameters = {'n_neighbors':[1,2,3,4,5,6], 'weights':['uniform','distance'], 'algorithm':['ball_tree','kd_tree']}\n",
        "\n",
        "# Make an fbeta_score scoring object\n",
        "scorer = make_scorer(fbeta_score, beta=2)\n",
        "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.4, random_state=0) \n",
        "cv  = sss.get_n_splits()\n",
        "# Perform grid search on the classifier using 'scorer' as the scoring method\n",
        "grid_obj = GridSearchCV(clf, parameters, scoring = scorer, cv = cv)\n",
        "\n",
        "# Fit the grid search object to the training data and find the optimal parameters\n",
        "grid_fit = grid_obj.fit(X_train, y_train)\n",
        "\n",
        "# Get the estimator\n",
        "best_clf = grid_fit.best_estimator_\n",
        "\n",
        "# Make predictions using the unoptimized and model\n",
        "predictions = (clf.fit(X_train, y_train)).predict(X_test)\n",
        "best_predictions = best_clf.predict(X_test)\n",
        "\n",
        "# Report the before-and-afterscores\n",
        "print (\"Unoptimized model\\n------\")\n",
        "print (\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_test, predictions)))\n",
        "print (\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, predictions, beta = 2)))\n",
        "print (\"\\nOptimized Model\\n------\")\n",
        "print (\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\n",
        "print (\"Final F-score on the testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 2)))\n",
        "print (\"\\n------\")\n",
        "print (\"Following are the parameters for the optimized model\")\n",
        "print (grid_fit.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D49khXo_Ivn"
      },
      "source": [
        "### Choosing best value for nearest neighbors\n",
        "Since, the grid search returns best value of number of nearest neighbors parameter as 1, we would like to separately iterate over number of neighbors to find the best value of ‘n_neighbors’. We do this to find how the accuracy and f-scores vary with increasing value of ‘n_neighbors’.\n",
        "\n",
        "When we do this, we see that accuracy and f-scores are same for all values of n_neighbors upto 10. This is further indication of the robustness of our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DCrQ8ew_Ivn"
      },
      "outputs": [],
      "source": [
        "acc_scores = []\n",
        "fbeta_scores = []\n",
        "k_range = range(1,16)\n",
        "for k in k_range:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train,y_train)\n",
        "    y_pred = knn.predict(X_test)\n",
        "    acc_scores.append(accuracy_score(y_test,y_pred))\n",
        "    fbeta_scores.append(fbeta_score(y_test, y_pred, beta=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krupBSIv_Ivo"
      },
      "outputs": [],
      "source": [
        "for i in range(len(acc_scores)):\n",
        "    print( \"{}: Accuracy is {}, f-score is {}\".format(i+1,acc_scores[i], fbeta_scores[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWHY3ygq_Ivp"
      },
      "source": [
        "### Featue Importance\n",
        "In this section, we would like to quantify the relative predictive power of each feature with respect to a given classifier. We choose two algorithms here, viz., Random Forests and Gradient Boosting, and determine the importance of each of the features.\n",
        "\n",
        "We see that first three features play a major role in predicting class labels, while entropy has the least predictive power."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T01tuAAY_Ivq"
      },
      "outputs": [],
      "source": [
        "# Import a supervised learning model that has 'feature_importances_'\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "clf = AdaBoostClassifier(random_state = 5)\n",
        "# Train the supervised model on the training set \n",
        "model = clf.fit(X_train, y_train)\n",
        "\n",
        "# Extract the feature importances\n",
        "importances = model.feature_importances_\n",
        "\n",
        "print (\"Feature Importances with regard to Random Forest Classifier:\")\n",
        "print (importances)\n",
        "print (\"Feature Importances with regard to Gradient Boosting Classifier:\")\n",
        "print (clf_C.feature_importances_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2lgnzqNM_Ivr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "banknote_authentication.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
